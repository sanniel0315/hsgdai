import datetime
import time
import os
import tarfile
import csv
import json
import re
import logging
from logging.handlers import TimedRotatingFileHandler
import shutil
import requests 
import glob
from tqdm import tqdm
from requests.auth import HTTPDigestAuth
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib3.exceptions import InsecureRequestWarning

# åœç”¨ InsecureRequestWarning è­¦å‘Š
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# ====== æ—¥èªŒè¨­å®šå‡½å¼ ======
def setup_logging():
    """è¨­å®šæ—¥èªŒè¨˜éŒ„å™¨ï¼Œä¸¦è¨­å®šä¿ç•™ä¸€å¹´çš„æ—¥èªŒè¼ªæ›¿"""
    log_filename = "daily_job.log"
    log_format = "%(asctime)s - %(levelname)s - %(message)s"
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    if logger.hasHandlers():
        logger.handlers.clear()
    
    file_handler = TimedRotatingFileHandler(
        log_filename, when='midnight', backupCount=365, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(file_handler)

    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(stream_handler)

# åœ¨ä¸»é‚è¼¯é–‹å§‹å‰å…ˆè¨­å®šå¥½ logging
setup_logging()

# ====== è¼‰å…¥è¨­å®šæª” ======
BASE_DIR = os.path.dirname(__file__)
try:
    with open(os.path.join(BASE_DIR, "config.json"), encoding="utf-8") as f:
        config = json.load(f)

    IPS = config['devices']['ips']
    USERNAME = config['devices']['credentials']['username']
    PASSWORD = config['devices']['credentials']['password']
    MAX_WORKERS = config['execution']['max_workers']
    DOWNLOAD_DIR = config['paths']['download_dir']

except FileNotFoundError:
    logging.critical("âŒ è‡´å‘½éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° config.json è¨­å®šæª”ï¼ç¨‹å¼ç„¡æ³•ç¹¼çºŒåŸ·è¡Œã€‚")
    exit()
except KeyError as e:
    logging.critical(f"âŒ è‡´å‘½éŒ¯èª¤ï¼šconfig.json æª”æ¡ˆä¸­ç¼ºå°‘å¿…è¦çš„éµå€¼ï¼š{e}ï¼ç¨‹å¼ç„¡æ³•ç¹¼çºŒåŸ·è¡Œã€‚")
    exit()

# ====== æ—¥æœŸæ™‚é–“å®šç¾© ======
run_datetime = datetime.datetime.now()
data_date = run_datetime.date() - datetime.timedelta(days=1)
run_date_str = run_datetime.strftime("%Y%m%d")
run_timestamp_str = run_datetime.strftime("%Y%m%d_%H%M")
data_date_str_for_filter = data_date.strftime("%Y-%m-%d")
data_date_str_for_filename = data_date.strftime("%Y-%m-%d")

# ====== è¼‰å…¥è¨­å‚™åµæ¸¬è¨­å®š ======
try:
    with open(os.path.join(BASE_DIR, "device_config.json"), encoding="utf-8") as f:
        DEVICE_CONFIG = json.load(f)
except FileNotFoundError:
    logging.info("â„¹ï¸ æ³¨æ„ï¼šæ‰¾ä¸åˆ° device_config.jsonï¼Œdetection_type å°‡å…¨éƒ¨é¡¯ç¤ºç‚ºã€ŒæœªçŸ¥ã€ã€‚")
    DEVICE_CONFIG = {}

os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# ====== å‡½å¼å®šç¾© ======

def process_logs(folder, ip, target_date_str):
    """è§£ææ—¥èªŒæª”æ¡ˆï¼Œå°‡çµæœå›å‚³ç‚ºä¸€å€‹åˆ—è¡¨ã€‚"""
    logging.info(f"ğŸ”· [{ip}] é–‹å§‹è§£æè³‡æ–™å¤¾: {folder}")
    all_rows = []
    summary = {}
    for root, _, files in os.walk(folder):
        for fname in files:
            if not fname.lower().endswith(".txt"): continue
            inp = os.path.join(root, fname)
            processed_lines_in_file = 0
            try:
                with open(inp, 'r', encoding="utf-8") as fin:
                    for line in fin:
                        parts = line.strip().split("~")
                        if len(parts) < 3: continue
                        device, ts = parts[0], parts[1]
                        if not ts.startswith(target_date_str): continue
                        try:
                            data = json.loads(parts[-2])
                        except json.JSONDecodeError: data = {}
                        for key in data.keys():
                            processed_lines_in_file += 1
                            spec_dir_name = data.get(key, {}).get("SpecDirName", "")
                            status = data.get(key, {}).get("status", "")
                            config_value = DEVICE_CONFIG.get(device, "æœªçŸ¥")
                            if isinstance(config_value, str): detection_type = config_value
                            elif isinstance(config_value, dict): detection_type = config_value.get(spec_dir_name, "æœªçŸ¥")
                            else: detection_type = "æœªçŸ¥"
                            all_rows.append([device, ts, spec_dir_name, status, detection_type])
                summary[fname] = processed_lines_in_file
            except Exception:
                logging.exception(f"âŒ [{ip}] è§£ææª”æ¡ˆ {fname} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼")
                summary[fname] = "Error"
    total_lines = len(all_rows)
    logging.info(f"âœ… [{ip}] å¾ {folder} ä¸­ç¸½å…±è§£æå‡º {total_lines} ç­†è³‡æ–™ã€‚")
    return {"data": all_rows, "summary": summary}


def download_archive(ip):
    """éšæ®µä¸€ï¼šä½¿ç”¨ requests å‡½å¼åº«ç›´æ¥å¾IPä¸‹è¼‰æª”æ¡ˆï¼ŒåŒ…å«èªè­‰ã€‚"""
    max_attempts = 2
    retry_delay = 60
    
    url = f"https://{ip}/pixord/model/aidataexport_3dago.php"
    temp_download_path = os.path.join(DOWNLOAD_DIR, f"aidata_{ip}_temp.tar.gz")
    
    for attempt in range(max_attempts):
        try:
            logging.info(f"ğŸ“¥ [{ip}] é–‹å§‹ç›´æ¥ä¸‹è¼‰ (ç¬¬ {attempt + 1} æ¬¡å˜—è©¦)...")

            # åœ¨ä¸‹è¼‰å‰ï¼Œåªæ¸…ç†å±¬æ–¼è‡ªå·±çš„é‚£å€‹è‡¨æ™‚æª”æ¡ˆ
            if os.path.exists(temp_download_path):
                os.remove(temp_download_path)

            with requests.get(url, auth=HTTPDigestAuth(USERNAME, PASSWORD), stream=True, verify=False, timeout=60) as r:
                r.raise_for_status()
                total = int(r.headers.get('content-length', 0))
                with open(temp_download_path, 'wb') as f, tqdm(
                    total=total, unit='B', unit_scale=True, unit_divisor=1024,
                    desc=f"ğŸ“¦ [{ip}] ä¸‹è¼‰ä¸­", ncols=80, leave=False
                ) as pbar:
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)
                        pbar.update(len(chunk))
                        
            # æª¢æŸ¥è‡¨æ™‚æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼Œç¢ºä¿ä¸‹è¼‰æˆåŠŸ
            if not os.path.exists(temp_download_path):
                raise FileNotFoundError("ä¸‹è¼‰å¾Œæœªæ‰¾åˆ°è‡¨æ™‚æª”æ¡ˆï¼Œå¯èƒ½ä¸‹è¼‰å¤±æ•—ã€‚")

            # é‡æ–°å‘½åæœ€çµ‚æª”æ¡ˆ
            dst = os.path.join(
                DOWNLOAD_DIR, f"aidata_{ip.replace('.', '_')}_{run_date_str}.tar.gz")
            if os.path.exists(dst):
                os.remove(dst)
            os.rename(temp_download_path, dst)

            logging.info(f"âœ… [{ip}] ç›´æ¥ä¸‹è¼‰å®Œæˆï¼š{dst}")
            return dst

        except requests.exceptions.RequestException as e:
            logging.warning(f"âš ï¸ [{ip}] ç¬¬ {attempt + 1} æ¬¡å˜—è©¦ä¸‹è¼‰å¤±æ•—: {e}")
            if attempt < max_attempts - 1:
                logging.info(f"â° [{ip}] ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦â€¦")
                time.sleep(retry_delay)
            else:
                logging.error(f"âŒ [{ip}] ä¸‹è¼‰é‡è©¦å¤±æ•—ï¼Œè·³éæ­¤ IPã€‚")
                return None
        except Exception as e:
            logging.error(f"âŒ [{ip}] è™•ç†ä¸‹è¼‰æ™‚ç™¼ç”Ÿéé æœŸéŒ¯èª¤: {e}")
            # ç¢ºä¿åœ¨ç™¼ç”Ÿéé æœŸéŒ¯èª¤æ™‚ä¹Ÿæ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if os.path.exists(temp_download_path):
                os.remove(temp_download_path)
            return None


def process_downloaded_file(archive_path):
    """éšæ®µäºŒï¼šè™•ç†å–®ä¸€å·²ä¸‹è¼‰çš„å£“ç¸®æª”ï¼Œä¸¦å›å‚³åŒ…å«IPå’Œè³‡æ–™çš„å­—å…¸ã€‚"""
    if not archive_path: return None
    filename = os.path.basename(archive_path)
    match = re.search(r'aidata_(.*?)_(\d{8})\.tar\.gz', filename)
    if not match:
        logging.error(f"âŒ ç„¡æ³•å¾æª”å {filename} è§£æå‡º IPã€‚")
        return None
    ip = match.group(1).replace('_', '.')
    run_date_from_name = match.group(2)
    logging.info(f"âš™ï¸ [{ip}] é–‹å§‹è™•ç†æª”æ¡ˆï¼š{filename}")
    try:
        ext_dir = os.path.join(DOWNLOAD_DIR, f"{ip}_{run_date_from_name}")
        os.makedirs(ext_dir, exist_ok=True)
        with tarfile.open(archive_path, "r:gz") as t:
            t.extractall(ext_dir)
        logging.info(f"ğŸ“‚ [{ip}] è§£å£“è‡³ï¼š{ext_dir}")
        result = process_logs(ext_dir, ip, data_date_str_for_filter)
        parsed_data = result.get("data", [])
        summary = result.get("summary", {})
        logging.info(f"ğŸ“Š [{ip}] æª”æ¡ˆè™•ç†ç­†æ•¸æ‘˜è¦ï¼š")
        if not summary:
            logging.info("   -> æœªè™•ç†ä»»ä½• .txt æª”æ¡ˆã€‚")
        for txt_file, count in summary.items():
            logging.info(f"   -> {txt_file}: {count} ç­†")
        logging.info(f"âœ”ï¸ [{ip}] æª”æ¡ˆè™•ç†å®Œç•¢ã€‚")
        return {"ip": ip, "data": parsed_data}
    except Exception:
        logging.exception(f"âŒ [{ip}] è™•ç†æª”æ¡ˆ {filename} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼")
        return None

if __name__ == "__main__":
    start_time = time.time()
    logging.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œè‡ªå‹•åŒ–è…³æœ¬ (åŸ·è¡Œæ™‚é–“: {run_datetime}, è³‡æ–™æ—¥: {data_date})")
    logging.info("\n" + "="*20 + " éšæ®µä¸€ï¼šé–‹å§‹å¹³è¡Œä¸‹è¼‰ " + "="*20)
    downloaded_files = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_ip = {executor.submit(download_archive, ip): ip for ip in IPS}
        for future in as_completed(future_to_ip):
            result_path = future.result()
            if result_path:
                downloaded_files.append(result_path)
    logging.info(f"\n" + "="*20 + f" éšæ®µäºŒï¼šé–‹å§‹è™•ç† {len(downloaded_files)} å€‹æª”æ¡ˆä¸¦åˆ†é¡è³‡æ–™ " + "="*20)
    data_by_device = {}
    device_to_ip_map = {}
    for archive_path in downloaded_files:
        result = process_downloaded_file(archive_path)
        if not result: continue
        ip = result["ip"]
        data_from_file = result["data"]
        for row in data_from_file:
            device_name = row[0]
            if device_name not in data_by_device:
                data_by_device[device_name] = []
                device_to_ip_map[device_name] = ip
            data_by_device[device_name].append(row)
    logging.info("\n" + "="*20 + " éšæ®µä¸‰ï¼šé–‹å§‹å¯«å…¥å„è¨­å‚™çš„ç¨ç«‹æª”æ¡ˆ " + "="*20)
    if not data_by_device:
        logging.info("â„¹ï¸ æ²’æœ‰ä»»ä½•è³‡æ–™å¯ä¾›å¯«å…¥ï¼Œç¨‹åºçµæŸã€‚")
    else:
        for device_name, rows in data_by_device.items():
            ip_address = device_to_ip_map.get(device_name, "UNKNOWN_IP")
            device_ip_folder = os.path.join(DOWNLOAD_DIR, ip_address)
            os.makedirs(device_ip_folder, exist_ok=True)
            final_csv_filename = f"{device_name}_{data_date_str_for_filename}_{run_timestamp_str}.csv"
            final_csv_path = os.path.join(device_ip_folder, final_csv_filename)
            try:
                with open(final_csv_path, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    writer.writerow(["device", "timestamp", "SpecDirName", "status", "detection_type"])
                    writer.writerows(rows)
                logging.info(f"ğŸ‰ [{device_name}] æˆåŠŸç”¢ç”Ÿå ±è¡¨ï¼å…± {len(rows)} ç­†è³‡æ–™ã€‚")
                logging.info(f"   å ±è¡¨è·¯å¾‘ï¼š{final_csv_path}")
            except Exception:
                logging.exception(f"âŒ [{device_name}] å¯«å…¥æœ€çµ‚ CSV æª”æ¡ˆæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼")
                
    logging.info("\n" + "="*20 + " éšæ®µå››ï¼šé–‹å§‹æ¸…ç†è‡¨æ™‚æª”æ¡ˆ " + "="*20)
    for archive_path in downloaded_files:
        filename = os.path.basename(archive_path)
        match = re.search(r'aidata_(.*?)_(\d{8})\.tar\.gz', filename)
        if match:
            ip = match.group(1).replace('_', '.')
            run_date_from_name = match.group(2)
            ext_dir = os.path.join(DOWNLOAD_DIR, f"{ip}_{run_date_from_name}")
            try:
                if os.path.isdir(ext_dir):
                    shutil.rmtree(ext_dir)
                    logging.info(f"ğŸ—‘ï¸ å·²åˆªé™¤è‡¨æ™‚è³‡æ–™å¤¾: {ext_dir}")
            except Exception as e:
                logging.error(f"âŒ åˆªé™¤è³‡æ–™å¤¾ {ext_dir} å¤±æ•—: {e}")
        try:
            if os.path.exists(archive_path):
                os.remove(archive_path)
                logging.info(f"ğŸ—‘ï¸ å·²åˆªé™¤å£“ç¸®æª”: {archive_path}")
        except Exception as e:
            logging.error(f"âŒ åˆªé™¤æª”æ¡ˆ {archive_path} å¤±æ•—: {e}")
    end_time = time.time()
    logging.info(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç¸½è€—æ™‚: {time.time() - start_time:.2f} ç§’")